{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR-fIzqxYZbr",
    "outputId": "efb59a6d-efc6-4ed4-9b57-e1402dcb9849"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch flask apscheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "X9AnTBU53PXT",
    "outputId": "8f9270f9-2148-41d3-ebac-4223ec41856b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Path to the MedQuAD dataset\n",
    "medquad_path = \"MedQuAD\" \n",
    "def parse_medquad(medquad_path):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    # Iterate through QA folders\n",
    "    for folder in os.listdir(medquad_path):\n",
    "        folder_path = os.path.join(medquad_path, folder)\n",
    "        if os.path.isdir(folder_path):  # Ignore files like license.txt and readme.txt\n",
    "            print(f\"Processing folder: {folder}\")  # Debugging\n",
    "            # Iterate through XML files in the folder\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".xml\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    try:\n",
    "                        tree = ET.parse(file_path)\n",
    "                        root = tree.getroot()\n",
    "                        \n",
    "                        # Parse questions and answers\n",
    "                        for qa in root.findall(\".//QAPairs/QAPair\"):\n",
    "                            question_elem = qa.find(\"Question\")\n",
    "                            answer_elem = qa.find(\"Answer\")\n",
    "                            \n",
    "                            # Ensure non-None values for question and answer\n",
    "                            question = question_elem.text if question_elem is not None else \"\"\n",
    "                            answer = answer_elem.text if answer_elem is not None else \"\"\n",
    "                            \n",
    "                            # Check if both question and answer are non-empty\n",
    "                            if question and question.strip() and answer and answer.strip():\n",
    "                                questions.append(question.strip())\n",
    "                                answers.append(answer.strip())\n",
    "                    except ET.ParseError:\n",
    "                        print(f\"Skipping malformed XML file: {file_path}\")  # Handle malformed XML\n",
    "\n",
    "    # Create a DataFrame\n",
    "    return pd.DataFrame({\"question\": questions, \"answer\": answers})\n",
    "\n",
    "# Parse the MedQuAD dataset\n",
    "medquad_df = parse_medquad(medquad_path)\n",
    "\n",
    "# Preview the dataset\n",
    "print(medquad_df.head())\n",
    "print(f\"Total QA pairs: {len(medquad_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset\n",
    "def convert_to_hf_dataset(df):\n",
    "    prompts = [f\"Question: {row['question']}\\nAnswer:\" for _, row in df.iterrows()]\n",
    "    completions = [f\" {row['answer']}\" for _, row in df.iterrows()]\n",
    "    return Dataset.from_dict({\"prompt\": prompts, \"completion\": completions})\n",
    "\n",
    "hf_dataset = convert_to_hf_dataset(medquad_df)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_test_split = hf_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "val_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Preview a sample from the dataset\n",
    "print(\"Dataset prepared:\")\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load GPT-2 and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_data(batch):\n",
    "    inputs = tokenizer(batch[\"prompt\"], truncation=True, max_length=256, padding=\"max_length\")  \n",
    "    labels = tokenizer(batch[\"completion\"], truncation=True, max_length=256, padding=\"max_length\")[\"input_ids\"]\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_data, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    no_cuda=True,  \n",
    ")\n",
    "\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install twilio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import sqlite3\n",
    "import datetime\n",
    "from pyngrok import ngrok\n",
    "import logging\n",
    "from twilio.rest import Client\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.start()\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Load the fine-tuned GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "\n",
    "# SQLite database setup\n",
    "conn = sqlite3.connect(\"healthcare.db\", check_same_thread=False)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS appointments (id INTEGER PRIMARY KEY, name TEXT, date TEXT, time TEXT)\"\n",
    ")\n",
    "cursor.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS reminders (id INTEGER PRIMARY KEY, name TEXT, medication TEXT, reminder_time TEXT)\"\n",
    ")\n",
    "#cursor.execute(\"ALTER TABLE reminders ADD COLUMN phone_number TEXT\")\n",
    "cursor.execute(\"PRAGMA table_info(reminders)\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return \"Welcome to the Healthcare Chatbot App!\"\n",
    "\n",
    "@app.route(\"/general_info\", methods=[\"POST\"])\n",
    "def general_info():\n",
    "    query = request.json.get(\"query\", \"\")\n",
    "    if not query:\n",
    "        return jsonify({\"error\": \"Query parameter is required\"}), 400\n",
    "\n",
    "    # Prepare input for GPT-2\n",
    "    input_text = f\"Please provide a detailed response to the following healthcare question:\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate response\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"],  # Pass attention mask\n",
    "        max_length=150,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,  # Enable sampling\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Explicitly set pad token\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return response\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "\n",
    "# Appointment booking\n",
    "@app.route(\"/book_appointment\", methods=[\"POST\"])\n",
    "def book_appointment():\n",
    "    data = request.json\n",
    "    name = data[\"name\"]\n",
    "    date = data[\"date\"]\n",
    "    time = data[\"time\"]\n",
    "    cursor.execute(\"INSERT INTO appointments (name, date, time) VALUES (?, ?, ?)\", (name, date, time))\n",
    "    conn.commit()\n",
    "    return jsonify({\"message\": f\"Appointment booked for {name} on {date} at {time}.\"})\n",
    "\n",
    "# Appointments Viewing\n",
    "@app.route(\"/search_appointments\", methods=[\"GET\"])\n",
    "def search_appointments():\n",
    "    # Retrieve query parameters\n",
    "    name = request.args.get(\"name\")\n",
    "    date = request.args.get(\"date\")\n",
    "    time = request.args.get(\"time\")\n",
    "\n",
    "    # Base query\n",
    "    query = \"SELECT * FROM appointments WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    # Add conditions dynamically based on provided parameters\n",
    "    if name:\n",
    "        query += \" AND name = ?\"\n",
    "        params.append(name)\n",
    "    if date:\n",
    "        query += \" AND date = ?\"\n",
    "        params.append(date)\n",
    "    if time:\n",
    "        query += \" AND time = ?\"\n",
    "        params.append(time)\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query, tuple(params))\n",
    "    appointments = cursor.fetchall()\n",
    "\n",
    "    # Format results as JSON\n",
    "    result = [\n",
    "        {\"id\": row[0], \"name\": row[1], \"date\": row[2], \"time\": row[3]}\n",
    "        for row in appointments\n",
    "    ]\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "@app.route(\"/set_reminder\", methods=[\"POST\"])\n",
    "def set_reminder():\n",
    "    data = request.json\n",
    "    name = data[\"name\"]\n",
    "    medication = data[\"medication\"]\n",
    "    reminder_time = data[\"reminder_time\"]\n",
    "    phone_number = data[\"phone_number\"]  \n",
    "\n",
    "    # Validate reminder_time\n",
    "    try:\n",
    "        reminder_time_obj = datetime.datetime.strptime(reminder_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if reminder_time_obj < datetime.datetime.now():\n",
    "            return jsonify({\"error\": \"Reminder time cannot be in the past\"}), 400\n",
    "    except ValueError:\n",
    "        return jsonify({\"error\": \"Invalid reminder time format. Use YYYY-MM-DD HH:MM:SS\"}), 400\n",
    "\n",
    "    # Save to database\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO reminders (name, medication, reminder_time, phone_number) VALUES (?, ?, ?, ?)\",\n",
    "        (name, medication, reminder_time, phone_number),\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    # Schedule the reminder\n",
    "    try:\n",
    "        scheduler.add_job(\n",
    "            func=send_sms_reminder,\n",
    "            trigger=\"date\",\n",
    "            run_date=reminder_time_obj,\n",
    "            args=[name, medication, phone_number],  # Pass the phone number\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Failed to schedule the reminder: {str(e)}\"}), 500\n",
    "\n",
    "    return jsonify({\"message\": f\"Reminder set for {name} to take {medication} at {reminder_time}.\"})\n",
    "\n",
    "#Remainders Viewing\n",
    "@app.route(\"/view_reminders\", methods=[\"GET\"])\n",
    "def view_reminders():\n",
    "    cursor.execute(\"SELECT * FROM reminders\")\n",
    "    reminders = cursor.fetchall()\n",
    "\n",
    "    # Convert the result into a list of dictionaries for easy JSON serialization\n",
    "    result = [\n",
    "        {\"id\": row[0], \"name\": row[1], \"medication\": row[2], \"reminder_time\": row[3]}\n",
    "        for row in reminders\n",
    "    ]\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "# Function to send SMS reminder\n",
    "def send_sms_reminder(name, medication, phone_number):\n",
    "    \n",
    "    # Twilio credentials\n",
    "    TWILIO_ACCOUNT_SID = os.getenv(\"TWILIO_ACCOUNT_SID\")\n",
    "    TWILIO_AUTH_TOKEN = os.getenv(\"TWILIO_AUTH_TOKEN\")  \n",
    "    \n",
    "    # Initialize the Twilio client globally\n",
    "    client = Client(account_sid, auth_token)\n",
    "    \n",
    "    # Message content\n",
    "    message_body = f\"Reminder: {name}, it's time to take your medication: {medication}.\"\n",
    "\n",
    "    # Send the SMS\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            body=message_body,\n",
    "            from_=\"+18449174827\",  \n",
    "            to=\"+17167300171\"\n",
    "        )\n",
    "        print(f\"Medication remainder sent to {phone_number}, SID: {message.sid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send SMS: {e}\")\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    public_url = ngrok.connect(5001)  \n",
    "    print(f\" * Ngrok Tunnel: {public_url}\") \n",
    "    app.run(host=\"0.0.0.0\", port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
